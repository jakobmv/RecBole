# Model Parameters
n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 2                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 384                # (int) The number of features in the hidden state.
inner_size: 1024                # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.2        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.2          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-12           # (float) A value added to the denominator for numerical stability.
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
mask_ratio: 0.2                 # (float) The probability for a item replaced by MASK token.
loss_type: 'DESS'               # (str) The type of loss function.
transform: mask_itemseq         # (str) The transform operation for batch data process.
ft_ratio: 0.5                   # (float) The probability of generating fine-tuning samples

# DESS Loss Parameters
beta: 1.0                       # (float) Beta parameter for DESS loss.
alpha: 0.5                      # (float) Alpha parameter for DESS loss.

# Training Parameters
epochs: 10                     # (int) The number of training epochs.
train_batch_size: 64           # (int) The training batch size.
eval_batch_size: 64            # (int) The evaluation batch size.
learning_rate: 0.001           # (float) Learning rate.
